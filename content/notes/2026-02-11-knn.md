---
title: KNN 算法实验记录
date: 2026-02-11
updated: 2026-02-15
excerpt: KNN 算法提要
categories:
  - slice
---

# 核心内涵
K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

## 需要思考的问题
### 选k
在算法应用中，k怎么确定的，k为多少效果最好呢？所谓的最近邻又是如何来判断给定呢？


- k 一般通过实验来选。


### 测量方法
- 逐像素差的平均欧氏距离
- 余弦相似度

### 预处理方法
- 边缘图代替原始像素值
  - 去除颜色、纹理、光照等变化，只保留形状结构
- 数据白化
- L2 归一化

# 相关知识 
## 训练集，验证集，测试集

* **训练集 (Training)**：**你的课本。** 占 60%-80%。你在这里反复磨练，允许犯错，目的是为了记住规律。
* **验证集 (Validation)**：**你的模拟考。** 占 10%-20%。用来调整参数（比如 k-NN 里的  取几）。它是“教练”，告诉你哪里需要微调。
* **测试集 (Test)**：**你的高考。** 占 10%-20%。它是“考官”，一旦碰到它，你就不能再改任何参数了，它是最后的终极真相。


##  计算距离


在图片空间中，设两张图像 $X, Y \in \mathbb{R}^{H \times W \times C}$（$H$: 高度, $W$: 宽度, $C$: 通道数）

其 **L1 距离** 定义为：

$$
d_{\text{L1}}(X, Y) = \|X - Y\|_1 = \sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{c=1}^{C} \bigl| X_{h,w,c} - Y_{h,w,c} \bigr|
$$

等价地，将图像展平为向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{HWC}$ 后：

$$
d_{\text{L1}}(X, Y) = \|\mathbf{x} - \mathbf{y}\|_1 = \sum_{i=1}^{HWC} |x_i - y_i|
$$

此即曼哈顿距离，衡量两张图像在像素级的绝对差异之和，对异常值比 L2 距离更鲁棒。

```python


```

---

其 **L2 距离** 定义为：

$$
d_{\text{L2}}(X, Y) = \|X - Y\|_2 = \sqrt{\sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{c=1}^{C} \bigl( X_{h,w,c} - Y_{h,w,c} \bigr)^2}
$$

等价地，将图像展平为向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{HWC}$ 后：

$$
d_{\text{L2}}(X, Y) = \|\mathbf{x} - \mathbf{y}\|_2 = \sqrt{(\mathbf{x} - \mathbf{y})^\top (\mathbf{x} - \mathbf{y})}
$$

此即欧几里得范数诱导的度量，衡量两张图像在像素级的全局差异。

---

其 **余弦距离** 定义为：

$$
d_{\cos}(X, Y) = 1 - \frac{\displaystyle\sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{c=1}^{C} X_{h,w,c} \, Y_{h,w,c}}
{\sqrt{\displaystyle\sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{c=1}^{C} X_{h,w,c}^2} \;
 \sqrt{\displaystyle\sum_{h=1}^{H} \sum_{w=1}^{W} \sum_{c=1}^{C} Y_{h,w,c}^2}}
$$

等价地（向量形式）：
$$
d_{\cos}(X, Y) = 1 - \frac{\mathbf{x}^\top \mathbf{y}}{\|\mathbf{x}\|_2 \, \|\mathbf{y}\|_2}
$$

取值范围 $[0, 2]$：值越小表示图像内容结构越相似（对整体亮度/对比度缩放不敏感）。

## 预处理方法

### 构造边缘图
设原始图像 $I \in \mathbb{R}^{H \times W \times C}$（$C \geq 1$ 为通道数）。通过边缘检测算子 $\mathcal{E}: \mathbb{R}^{H \times W \times C} \to \mathbb{R}^{H \times W}$，将其映射为**边缘图** $E = \mathcal{E}(I)$，定义为：

$$
E_{h,w} = \left\| \nabla I_{\text{gray}}(h,w) \right\|_2
= \sqrt{ \left( \frac{\partial I_{\text{gray}}}{\partial x} \right)^2 + \left( \frac{\partial I_{\text{gray}}}{\partial y} \right)^2 }
$$

其中：
- $I_{\text{gray}} \in \mathbb{R}^{H \times W}$ 为灰度化图像（如 $I_{\text{gray}} = 0.299 I_R + 0.587 I_G + 0.114 I_B$），
- $\nabla$ 为梯度算子（常用 Sobel、Scharr 等离散近似）。

---

-  关键性质
   1. **去除颜色**：通过灰度化 $I \to I_{\text{gray}}$ 消除通道差异。
   2. **去除光照**：梯度 $\nabla I_{\text{gray}}$ 对全局亮度偏移 $\Delta$ 不敏感（$\nabla    (I_{\text{gray}} + \Delta) = \nabla I_{\text{gray}}$）。
   3. **抑制纹理**：边缘检测聚焦**显著梯度变化**（物体边界），平滑区域与高频纹理被抑制。
   4. **保留形状**：$E$ 仅编码图像中**强度突变的位置与方向**，即物体轮廓与结构。

>  边缘图 $E$ 是原始图像的**几何结构表示**，对颜色、光照、均匀纹理变化具有不变性。


### 数据白化

设数据矩阵 $\mathbf{X} \in \mathbb{R}^{n \times d}$（$n$ 个样本，$d$ 维特征），**白化**是线性变换：

$$
\mathbf{Z} = (\mathbf{X} - \boldsymbol{\mu}) \mathbf{W}, \quad \text{满足} \quad \frac{1}{n} \mathbf{Z}^\top \mathbf{Z} = \mathbf{I}_d
$$

其中：
- $\boldsymbol{\mu} = \frac{1}{n} \sum_{i=1}^n \mathbf{x}_i$（样本均值向量）
- $\mathbf{W} = \mathbf{U} \mathbf{\Lambda}^{-1/2} \mathbf{U}^\top$（ZCA白化矩阵）
- $\mathbf{\Sigma} = \frac{1}{n} (\mathbf{X} - \boldsymbol{\mu})^\top (\mathbf{X} - \boldsymbol{\mu}) = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^\top$（协方差矩阵的特征分解）

**效果**：白化后数据 $\mathbf{Z}$ 满足：
- 零均值：$\mathbb{E}[\mathbf{z}] = \mathbf{0}$
- 单位方差：$\operatorname{Var}(z_j) = 1$
- 无相关性：$\operatorname{Cov}(z_j, z_k) = 0 \;(j \neq k)$

### L2 归一化

**定义**：  
对非零向量 $\mathbf{x} = [x_1, x_2, \dots, x_n]^\top \in \mathbb{R}^n$，其 **L2 归一化** 定义为：
$$
\hat{\mathbf{x}} = \frac{\mathbf{x}}{\|\mathbf{x}\|_2}, \quad \text{其中} \quad \|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2} > 0
$$
归一化后满足 $\|\hat{\mathbf{x}}\|_2 = 1$。


- 关键性质
- 保持方向不变：$\hat{\mathbf{x}}$ 与 $\mathbf{x}$ 共线（$\hat{\mathbf{x}} = k\mathbf{x},\ k>0$）。
- 仅当 $\mathbf{x} \neq \mathbf{0}$ 时定义（零向量无 L2 归一化）。
- 广泛应用于机器学习（如特征标准化、余弦相似度计算）。
